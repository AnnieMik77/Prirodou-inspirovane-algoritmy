{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radial Basics Functions a Recurrent Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minule jsme si ukazovali a zkoušeli si naprogramovat jednoduché neuronové sítě. Dneska se podíváme na složitější struktury neuronových sítí Radial Basics Functions a rekurentní neuronové sítě.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Basics Functions (RBF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naimplementujeme si nyní RBF síť. Implementace v tensorflow je jednoduchá -- stačí naimplementovat třídu, která bude mít dvě metody. Metoda *build* nainicializuje parametry podle velikosti vstupu a metoda *call* pak implementuje vlastní výpočet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katie\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFNetwork():\n",
    "    def __init__(self, input_dim, num_centers, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_centers = num_centers\n",
    "        self.centers = [np.random.uniform(-1, 1, input_dim) for i in range(num_centers)]\n",
    "        self.beta = 1\n",
    "        self.weights = None\n",
    "        \n",
    "    # spocteni hodnot neuronu na skryte vrstve\n",
    "    def calculate_activation(self, data):\n",
    "        hidden_layer_values = np.zeros((data.shape[0], self.num_centers), float)\n",
    "        for c_idx, c in enumerate(self.centers):\n",
    "            for x_idx, x in enumerate(data):\n",
    "                hidden_layer_values[x_idx,c_idx] = self.activation_fcn(c, x)\n",
    "        return hidden_layer_values\n",
    "    \n",
    "    #  hodnota aktivacni fce\n",
    "    def activation_fcn(self, centers, data):\n",
    "        return np.exp(-self.beta * np.linalg.norm(centers-data)**2)\n",
    "    \n",
    "    def fit(self, data, labels):\n",
    "        # zvol nahodne hodnoty pocatecnich centroidu\n",
    "        random_idx = np.random.permutation(data.shape[0])[:self.num_centers]\n",
    "        self.centers = [data[i,:] for i in random_idx]\n",
    "         \n",
    "        # spocitame aktivaci mezi vstupem a skrytou vrstvou\n",
    "        hidden_layer_values = self.calculate_activation(data)\n",
    "         \n",
    "        # porovname skutecne a predikovane vystupy a aktualizujem vahy \n",
    "        # pseudoinverzni matice je vlastne vzorecek pro LR pro train vah\n",
    "        self.weights = np.dot(np.linalg.pinv(hidden_layer_values), labels)\n",
    "          \n",
    "    def predict(self, data):\n",
    "        hidden_layer_values = self.calculate_activation(data)\n",
    "        labels = np.dot(hidden_layer_values, self.weights)\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zkusíme si naši RBF síť pustit na našem oblíbeném datasetu Iris. Načteme si data a labely, do které třídy data patří. Protože budeme dělat klasifikaci, je vhodné si labely převést na one-hot-encoding. Následně data rozdělíme na trénovací a testovací množinu, abychom mohli zvlášť data trénovat a zvlášť data testovat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# ziskame data\n",
    "iris = datasets.load_iris()\n",
    "x, y = iris.data, iris.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "\n",
    "# delame klasifikaci prevedeme vystup do one hot encoding\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0]) \n",
    "y_train_onehot = onehotencoder.fit_transform(y_train.reshape(-1, 1)).toarray() \n",
    "y_test_onehot = onehotencoder.fit_transform(y_test.reshape(-1, 1)).toarray() \n",
    "\n",
    "# natrenujeme a ohodnotime sit\n",
    "rbf = RBFNetwork(4, 10, 3)\n",
    "rbf.fit(x_train, y_train_onehot)\n",
    "predicted = rbf.predict(x_test)\n",
    "y_pred = np.argmax(predicted, axis=1)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print('Accuracy: ' + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidíme, že accuracy je v každém běhu dosti různá, což je způsobeno tím, že centroidy a betu zde nastavujeme náhodně, přestože jsme si říkali, že pozice centroidů se dá trénovat pomocí algoritmu k-means.\n",
    "\n",
    "### Úkol na cvičení\n",
    "\n",
    "Zkuste si naimplementovat algoritmus k-means pro inicializaci středů vstupních neuronů a zlepšit tím výstup sítě výše. Hint: metoda `add_variable` má parametr initializer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rekurentní neuronové sítě\n",
    "\n",
    "Rekurentní neuronová síť je síť, která navíc ke svému vstupu ještě bere jako další vstup svůj výstup z předchozího kroku. Proto výstupy z předchozí vrstvy mohou ovlivňovat vrstvu následující, což se může hodit například u generování časových řad nebo textu. Nejprve se podíváme, jak přesně by vypadala implementace jednoduché RNN. Informace z předchozího kroku se ukládá v RNN buňce do speciální proměnné -- tzv. skrytého stavu, který nám pak bude ovlivňovat další výstupy a bude se s každým novým vstupem v každém kroku aktualizovat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RNN' object has no attribute 'W_hh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e9a2b0f131c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mrnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-e9a2b0f131c0>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# update skrytého stavu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_hh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_xh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;31m# vypocet vystupniho vektoru\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW_hy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RNN' object has no attribute 'W_hh'"
     ]
    }
   ],
   "source": [
    "class RNN:\n",
    "    def step(self, x):\n",
    "    # update skrytého stavu\n",
    "        self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "        # vypocet vystupniho vektoru\n",
    "        y = np.dot(self.W_hy, self.h)\n",
    "        return y\n",
    "\n",
    "rnn = RNN()\n",
    "y = rnn.step(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sekvenční klasifikace pomocí LSTM\n",
    "\n",
    "Nyní, když chápeme, jak taková základní RNN funguje, zkusíme se podívat na složitější druh RNN -- LSTM sítě. Tyto sítě mají navíc uvnitř sebe paměťovou buňku a mechanizmus, který řídí,  jakou informaci si buňka pamatuje a jakou zapomíná. Zkusíme se na ni lépe podívat v následujícím příkladu sekvenční klasifikace. \n",
    "\n",
    "Sekvenční klasifikace je prediktivní modelovací problém, kdy máme na vstupu nějakou sekvenci v prostoru nebo čase a cílem je předpovědět kategorii této sekvence. Složitost tohoto problému spočívá v tom, že jednotvlivé sekvence mohou mít různou délku nebo mohou být složeny z rozsáhlého slovníku vstupních hodnot a mohou vužadovat, aby se model naučil nějaké dlouhodobé závislosti nebo kontext mezi vstupními sekvencemi.\n",
    "\n",
    "Zkusíme se tedy podívat na příklad sekvenční klasifikace pomocí LSTM na IMDB datasetu, což je dataset, který obsahuje slovní popis recenzí 50K filmů a následně klasifikaci, jestli byla recenze pozitivní nebo negativní v poměru 1:1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# zafixujem random seed pro reprodukovatelnost\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problém je, že slovní popis je nějak potřeba převést na číselnou reprezentaci. Naštěstí funkce ```imdb.load_data``` umí načíst data tak, že rovnou slova nahradí čísly a rozdělí je na train a test množiny v poměru 1:1. Navíc data načteme tak, že necháme jen prvních ```top_words``` nejčastějších slova  zbytek nahradíme 0. Dále je potřeba zkrátit nebo doplnit vstupní sekvence pro modelování tak, aby byly všechny stejně dlouhé, délku nastavíme na ```max_len```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "\n",
    "max_length = 500\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nyní máme připravená data a můžeme si definovat a natrénovat model.\n",
    " - První vrstva je ```Embedding```, která používá vektory délky 32 pro každé slovo. \n",
    " - Další vrstva je ```LSTM``` vrstva, která obsahuje 100 paměťových jednotek (neuronů). \n",
    " - Na závěr použijeme ```Dense``` výstupní vrstvu s jedním neuronem a aktivační funkcí sigmoid k vytvoření predikcí 0 nebo 1, protože se jedná o klasifikační úlohu.\n",
    "\n",
    "Problém modelu je, že se velice snadno overfittuje na na daná trénovací data. Proto se se používají ještě vrstvy ```Dropout```, které spočívají v tom, že během trénování se náhodně vynechávají některé vstupy do další vrstvy. Tím se simuluje velký počet sítí s odlišnou strukturou a uzly jsou pak robustnější a omezuje se tím pádem overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 484s 19ms/step - loss: 0.4778 - acc: 0.7633\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 519s 21ms/step - loss: 0.2916 - acc: 0.8830\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 544s 22ms/step - loss: 0.3016 - acc: 0.8745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2580d55da20>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na závěr zkusíme predikovat výstupy na testovacích datech a podívat se, jak je model dobrý. Můžete si třeba zkusit pustit trénování modelu s droupoutem a bez něj a podívat se, jak se budou lišit výsledné accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.14%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generování textu znak po znaku \n",
    "\n",
    "Nyní se podíváme na jiný druh problému -- budeme generovat text znak po znaku, neboli natrénujeme jazykový model tak, že když mu pak dáme sekvenci znaků, tak nám model bude schopný předpovědět další znak. Jako trénovací množinu použijeme texty Nietzscheho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Example script to generate text from Nietzsche's writings.\n",
    "    At least 20 epochs are required before the generated text\n",
    "    starts sounding coherent.\n",
    "    It is recommended to run this script on GPU, as recurrent\n",
    "    networks are quite computationally intensive.\n",
    "    If you try this script on new data, make sure your corpus\n",
    "    has at least ~100k characters. ~1M is better.\n",
    "'''\n",
    "\n",
    "# nacteme si vstupni data\n",
    "path = tf.keras.utils.get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = set(text)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# rozdelime text na castecne zavisle sekvence znaku delky maxlen\n",
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "# prevedeme text na ciselne vektory\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "# vytvorime si model    \n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars), activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# pomocna funkce k ziskani indexu z pravdepodobnostniho pole\n",
    "def sample(a, temperature=1.0):  \n",
    "    a = np.log(a) / temperature\n",
    "    a = np.exp(a) / np.sum(np.exp(a))\n",
    "    a = a/np.sum(a)\n",
    "    return np.argmax(np.random.multinomial(1, a, 1))\n",
    "\n",
    "# natrenujeme model a po kazde iterace generujeme vystup\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y, batch_size=128, nb_epoch=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for _ in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skript si samozřejmě můžete pustit, ale trénování poběží neskutečně dlouho. Proto jsme skript pustili na Google Colab a na výsledky se můžete podívat [zde](https://colab.research.google.com/drive/1B7zys275xmpPqahPwNvuYMPLmgvlV3l5) nebo [zde](/notebooks/07_rbf_rnn/results.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
